{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pickle\n",
    "import sys\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_liste = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article:\n",
    "    def __init__(self, title, lead, datum, author, text, url):\n",
    "        self.title = title\n",
    "        self.lead = lead\n",
    "        self.datum = datum\n",
    "        self.author = author\n",
    "        self.text = text\n",
    "        self.url = url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inhalter():\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    try:\n",
    "        title = soup.find(\"h1\").get_text().strip()\n",
    "    except AttributeError:\n",
    "        title = \"\"\n",
    "    \n",
    "    try:\n",
    "        lead = soup.find(class_=\"field-subhead\").get_text().strip()\n",
    "    except AttributeError:\n",
    "        lead = \"\"\n",
    "    \n",
    "    try:\n",
    "        author = soup.find(class_=\"authorline__link--archiv\").get_text().strip()\n",
    "    except AttributeError:\n",
    "        author = \"\"\n",
    "    \n",
    "    try:\n",
    "        date = soup.find(class_=\"field-issue-ref\").get_text().strip()\n",
    "        date = date[-11:].strip()\n",
    "    except AttributeError:\n",
    "        date = \"\"\n",
    "    \n",
    "    fintext = str()\n",
    "    text = soup.find_all(\"p\", class_=None)\n",
    "    for i in text[:-5]:\n",
    "        if author not in i.get_text():\n",
    "            i = i.get_text()\n",
    "            i = i.replace(\"\\n\", \" \")\n",
    "            i = i.strip()\n",
    "            fintext += i\n",
    "            \n",
    "    ding_url = page\n",
    "    \n",
    "    crawled = Article(title, lead, date, author, fintext, ding_url)\n",
    "    crawled_liste.append(crawled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox()\n",
    "driver.implicitly_wait(30)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.woz.ch/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual login\n",
    "#p = pickle.dump(driver.get_cookies() , open(\"woz_cookies.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cookies = pickle.load(open(\"woz_cookies.pkl\", \"rb\"))\n",
    "for cookie in cookies:\n",
    "    driver.add_cookie(cookie)\n",
    "    \n",
    "driver.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"woz_output.bin\", \"rb\") as data:\n",
    "    ueb_pages = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in ueb_pages:\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        driver.get(page)\n",
    "        inhalter()\n",
    "    except TimeoutException:\n",
    "        print(\"TimeOut\")\n",
    "        pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('woz_art1.csv', 'w', newline='', encoding=\"utf8\") as csvfile:\n",
    "    blogwriter = csv.writer(csvfile, delimiter='|', quotechar='\"')\n",
    "\n",
    "\n",
    "    for article in crawled_liste:\n",
    "        blogwriter.writerow( [article.title, article.lead, article.datum, article.author, article.text, article.url] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
