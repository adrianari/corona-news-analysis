{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pickle\n",
    "import sys\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_liste = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article:\n",
    "    def __init__(self, title, lead, datum, zeit, author, text, url):\n",
    "        self.title = title\n",
    "        self.lead = lead\n",
    "        self.datum = datum\n",
    "        self.zeit = zeit\n",
    "        self.author = author\n",
    "        self.text = text\n",
    "        self.url = url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox()\n",
    "driver.implicitly_wait(30)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_seite = \"https://www.luzernerzeitung.ch/\"\n",
    "driver.get(start_seite)\n",
    "\n",
    "cookies = pickle.load(open(\"nlz_cookies.pkl\", \"rb\"))\n",
    "for cookie in cookies:\n",
    "    driver.add_cookie(cookie)\n",
    "    \n",
    "driver.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nlz_output.bin\", \"rb\") as data:\n",
    "    ueb_pages = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "for element in ueb_pages:\n",
    "    urlf = \"https://www.luzernerzeitung.ch\" + element\n",
    "    urls.append(urlf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inhalter():\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    try:\n",
    "        title = soup.find(class_=\"headline__title\")\n",
    "        title = title.get_text() \n",
    "        title = title.replace(\"\\n\", \" \")\n",
    "        title = title.strip()\n",
    "    except AttributeError:\n",
    "        print(\"AttributError\")\n",
    "        pass\n",
    "    print(title)\n",
    "\n",
    "    try:\n",
    "        lead = soup.find(class_=\"headline__lead\").get_text()\n",
    "        lead = lead.replace(\"\\n\", \" \")\n",
    "    except AttributeError:\n",
    "        lead = \"\"\n",
    "\n",
    "    try:\n",
    "        author = soup.find(class_=\"metainfo__item metainfo__item--author\").get_text()\n",
    "    except AttributeError:\n",
    "        author = \"\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        time_indy = soup.find(class_=\"metainfo__item metainfo__item--date\").get_text().strip()\n",
    "        time_indy = time_indy.split(\",\")\n",
    "        time = time_indy[1].strip()\n",
    "        if \"Uhr\" in time:\n",
    "            time = time.replace(\"Uhr\", \"\")\n",
    "        date = time_indy[0]\n",
    "    except AttributeError:\n",
    "        time = \"\"\n",
    "        date = \"\"\n",
    "\n",
    "    texte = soup.find_all(\"p\", class_=re.compile(\"articlecomponent.text\"))\n",
    "    artikel = []\n",
    "    for element in texte:\n",
    "        element = element.get_text()\n",
    "        artikel.append(element)\n",
    "\n",
    "    article = \" \".join(artikel)\n",
    "    \n",
    "    durl = page\n",
    "    \n",
    "    crawled = Article(title, lead, date, time, author, article, durl)\n",
    "    crawled_liste.append(crawled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in urls:\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        driver.get(page)\n",
    "        inhalter()\n",
    "    except TimeoutException:\n",
    "        print(\"TimeOut\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nlz_art1.csv', 'w', newline='', encoding=\"utf8\") as csvfile:\n",
    "    blogwriter = csv.writer(csvfile, delimiter='|', quotechar='\"')\n",
    "\n",
    "\n",
    "    for article in crawled_liste:\n",
    "        blogwriter.writerow( [article.title, article.lead, article.datum, article.zeit, article.author, article.text, article.url] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
