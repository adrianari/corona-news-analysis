{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have not imported the dnspython module\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pickle\n",
    "import sys\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_liste = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article:\n",
    "    def __init__(self, title, lead, datum, zeit, author, text, url):\n",
    "        self.title = title\n",
    "        self.lead = lead\n",
    "        self.datum = datum\n",
    "        self.zeit = zeit\n",
    "        self.author = author\n",
    "        self.text = text\n",
    "        self.url = url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inhalter():    \n",
    "    #main\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    #segments\n",
    "    try:\n",
    "        title = soup.find(class_=re.compile(\"title nau-t-typography-display-1\")).get_text()\n",
    "        title = title.strip()\n",
    "    except AttributeError:\n",
    "        title = \"\"\n",
    "\n",
    "    ## lead (as it is not optional to have one)\n",
    "    try:\n",
    "        lead = soup.find(class_=re.compile(\"lead nau-t-typography-display-3\")).get_text()\n",
    "        lead = lead.strip()\n",
    "    except AttributeError:\n",
    "        lead = \"\"\n",
    "\n",
    "    # datum (is not always given)\n",
    "    try:\n",
    "        datum = soup.find(class_=re.compile(\"date\")).get_text()\n",
    "        zeit = datum[-5:]\n",
    "        zeit = zeit.strip()\n",
    "        datum = datum.replace(\"Am \", \"\")\n",
    "        datum = datum[:-8]\n",
    "        datum = datum.strip()\n",
    "    except AttributeError:\n",
    "        datum = \"\"\n",
    "        zeit = \"\"\n",
    "\n",
    "    #author\n",
    "    try:\n",
    "        author = soup.find(class_=re.compile(\"author-name ng-star-inserted\")).get_text()\n",
    "    except AttributeError:\n",
    "        author = \"\"\n",
    "    if \"Beitrag von\" in author:\n",
    "        author = author.replace(\"Beitrag von \", \"\")\n",
    "        author = author.strip()\n",
    "    \n",
    "    \n",
    "    text = str()\n",
    "    blocktext = soup.find_all(class_=re.compile(\"nau-t-typography\"))\n",
    "    for teil in blocktext:\n",
    "        texty = teil.get_text()\n",
    "        texty = texty.replace('\\t','')\n",
    "        texty = texty.replace('\\n','')\n",
    "        text += \" \" + texty\n",
    "    if \"Mehr zum Thema\" in text:\n",
    "        p = text.index(\"Mehr zum Thema\")\n",
    "        text = text[:p]\n",
    "        text = text.strip()\n",
    "    if \".  \" in text:\n",
    "        text = text.replace(\".  \", \". \")\n",
    "    ding_url = page\n",
    "    \n",
    "    crawled = Article(title, lead, datum, zeit, author, text, ding_url)\n",
    "    crawled_liste.append(crawled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox()\n",
    "driver.implicitly_wait(30)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_seite = \"https://www.nau.ch/politik/regional/prozess-gegen-pierre-maudet-in-genf-eroffnet-65871426\"\n",
    "driver.get(start_seite)\n",
    "\n",
    "cookies = pickle.load(open(\"nau_cookies.pkl\", \"rb\"))\n",
    "for cookie in cookies:\n",
    "    driver.add_cookie(cookie)\n",
    "    \n",
    "driver.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nau_output.bin\", \"rb\") as data:\n",
    "    ueb_pages = pickle.load(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in ueb_pages[9460:]:\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        driver.get(page)\n",
    "        inhalter()\n",
    "    except TimeoutException:\n",
    "        print(\"TimeOut\")\n",
    "        pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nau_art.csv', 'w', newline='', encoding=\"utf8\") as csvfile:\n",
    "    blogwriter = csv.writer(csvfile, delimiter='|', quotechar='\"')\n",
    "\n",
    "\n",
    "    for article in crawled_liste:\n",
    "        blogwriter.writerow( [article.title, article.lead, article.datum, article.zeit, article.author, article.text, article.url] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick fix\n",
    "In case one needs to interrupt crawling and wants to continue later where the crawler stopped, run the cell below and update cell 7, first line from: <br>\n",
    "*\"for page in ueb_pages:\"* <br>\n",
    "to : <br>\n",
    "*\"for page in ueb_pages[go_on:]:\"*\n",
    "\n",
    "This method also works in case of uncatched errors or interrupted kernels, provided kernel has not been restarted yet. In case kernel will be restarted, please note down the value of go_on or pickle dump the value beforehand as the value of go_on will not be stored automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
