{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pickle\n",
    "import sys\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_liste = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article:\n",
    "    def __init__(self, title, lead, datum, author, text, url):\n",
    "        self.title = title\n",
    "        self.lead = lead\n",
    "        self.datum = datum\n",
    "        self.author = author\n",
    "        self.text = text\n",
    "        self.url = url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inhalter():\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    title = soup.find(class_=\"ContentHead_text__NId_w\")\n",
    "    title = title.get_text()\n",
    "    title = title.strip()\n",
    "\n",
    "        \n",
    "    try:\n",
    "        lead = soup.find(class_=\"ContentHead_lead__1S033\")\n",
    "        lead = lead.get_text()\n",
    "    except AttributeError:\n",
    "        lead = \"\"\n",
    "    \n",
    "    try:\n",
    "        author = soup.find(class_=\"ContentMetaInfo_author__3hPjj\")\n",
    "        author = author.get_text()\n",
    "    except AttributeError:\n",
    "        author = \"\"\n",
    "        \n",
    "    try:\n",
    "        time_indy = soup.find(class_=re.compile(\"FullDateTime_root__35h8M ArticleContainer_content-width__pYdH3\"))\n",
    "        time_indy = time_indy.get_text()\n",
    " #       time_indy = time_indy.split(\",\")\n",
    " #       time = time_indy[1]\n",
    " #       time = time.get_text()\n",
    " #       date = time_indy[0]\n",
    " #       date = date.get_text()\n",
    "    except AttributeError:\n",
    "        time_indy = \"\"\n",
    " #       time = \"\"\n",
    " #       date = \"\"\n",
    "\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        text = str()\n",
    "        ts = soup.find_all(class_=\"HtmlText_root__2zbjl\")\n",
    "        for t in ts:\n",
    "            t = t.get_text()\n",
    "            t = t.replace(\"\\n\", \" \")\n",
    "            t = t.strip()\n",
    "            text += t\n",
    "    except AttributeError:\n",
    "        text = \"\"\n",
    "        \n",
    "    url = page          \n",
    "    \n",
    "    crawled = Article(title, lead, time_indy, author, text, url)\n",
    "    crawled_liste.append(crawled)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox()\n",
    "driver.implicitly_wait(30)\n",
    "driver.maximize_window()\n",
    "\n",
    "driver.get(\"https://www.tagesanzeiger.ch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manueller Login via driver GUI\n",
    "#p = pickle.dump(driver.get_cookies() , open(\"tagi_cookies.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cookies = pickle.load(open(\"tagi_cookies.pkl\", \"rb\"))\n",
    "for cookie in cookies:\n",
    "    driver.add_cookie(cookie)\n",
    "    \n",
    "driver.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tagesanzeiger_output.bin\", \"rb\") as data:\n",
    "    ueb_pages = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in ueb_pages:\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        driver.get(page)\n",
    "        inhalter()\n",
    "    except TimeoutException:\n",
    "        print(\"TimeOut\")\n",
    "        pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tagi_art1.csv', 'w', newline='', encoding=\"utf8\") as csvfile:\n",
    "    blogwriter = csv.writer(csvfile, delimiter='|', quotechar='\"')\n",
    "\n",
    "\n",
    "    for article in crawled_liste:\n",
    "        blogwriter.writerow( [article.title, article.lead, article.datum, article.author, article.text, article.url] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
