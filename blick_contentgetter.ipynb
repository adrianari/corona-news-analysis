{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code bases upon the assumption that blick_sitegetter has been executed. The below code will get all data from the defined Blick sites:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pickle\n",
    "import sys\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_liste = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article:\n",
    "    def __init__(self, title, lead, datum, zeit, author, text, url):\n",
    "        self.title = title\n",
    "        self.lead = lead\n",
    "        self.datum = datum\n",
    "        self.zeit = zeit\n",
    "        self.author = author\n",
    "        self.text = text\n",
    "        self.url = url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox()\n",
    "driver.implicitly_wait(30)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_seite = \"https://www.blick.ch/\"\n",
    "driver.get(start_seite)\n",
    "\n",
    "cookies = pickle.load(open(\"blick_cookies.pkl\", \"rb\"))\n",
    "for cookie in cookies:\n",
    "    driver.add_cookie(cookie)\n",
    "    \n",
    "driver.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"blick_output.bin\", \"rb\") as data:\n",
    "    ueb_pages = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inhalter():    \n",
    "    driver.get(page)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    \n",
    "    title = soup.find(class_=\"title\")\n",
    "    title = title.get_text()\n",
    "\n",
    "    try:\n",
    "        lead = soup.find(class_=\"article-lead\")\n",
    "        lead = lead.get_text()\n",
    "    except AttributeError:\n",
    "        lead = \"\"\n",
    "\n",
    "    try:\n",
    "        datum = soup.find(class_=re.compile(\"ArticleDate__Wrapper\"))\n",
    "        datum = datum.get_text()\n",
    "        zeit = datum[-10:-3].strip()\n",
    "        datum = datum[12:-11].strip()\n",
    "    except AttributeError:\n",
    "        datum = \"\"\n",
    "        zeit = \"\"\n",
    "\n",
    "    try:\n",
    "        author = soup.find(class_=re.compile(\"detail__item__header__item\"))\n",
    "        author.get_text().strip()\n",
    "    except AttributeError:\n",
    "        author = \"\"\n",
    "\n",
    "    try:\n",
    "        text = \"\"\n",
    "        texte = soup.find_all(\"p\")\n",
    "        for x in texte:\n",
    "            x = x.get_text()\n",
    "            text += x\n",
    "    except AttributeError:\n",
    "        text = \"\"\n",
    "        \n",
    "    durl = page\n",
    "    \n",
    "    crawled = Article(title, lead, datum, zeit, author, text, durl)\n",
    "    crawled_liste.append(crawled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in ueb_pages[5703:]:\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        inhalter()\n",
    "    except TimeoutException:\n",
    "        print(\"TimeOut\")\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('blick_art4.csv', 'w', newline='', encoding=\"utf8\") as csvfile:\n",
    "    blogwriter = csv.writer(csvfile, delimiter='|', quotechar='\"')\n",
    "\n",
    "\n",
    "    for article in crawled_liste:\n",
    "        blogwriter.writerow( [article.title, article.lead, article.datum, article.zeit, article.author, article.text, article.url] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick fix\n",
    "In case one needs to interrupt crawling and wants to continue later where the crawler stopped, run this cell and update cell 7, first line from: <br>\n",
    "*\"for page in ueb_pages:\"* <br>\n",
    "to : <br>\n",
    "*\"for page in ueb_pages[go_on:]:\"*\n",
    "\n",
    "This method also works in case of uncatched errors or interrupted kernels, provided kernel has not been restarted yet. In case kernel will be restarted, please note down the value of go_on or pickle dump the value beforehand as the value of go_on will not be stored automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = crawled_liste[-1].url    ##get the url from the last object that has been scraped\n",
    "go_on = ueb_pages.index(latest)   ##find this url in the list that is crawled through and extract its index\n",
    "print(go_on)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
